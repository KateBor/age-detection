{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Эти два импорта используются для работы с изображениями,\n",
    "# в том числе загрузкой готовых датасетов, их трансформированием и подготовкой к обучению модели.\n",
    "\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подготовка датасета для обучения модели.\n",
    "# - Загружаем датасет CelebA - это большой датасет для обучения в области распознавания лиц.\n",
    "# - split=\"train\" - выбираем часть датасета, которая содержит тренировочные данные.\n",
    "# - transforms.Compose - определяем последовательность преобразований, которые должны быть применены к изображениям перед их использованием в обучении\n",
    "#    - transforms.CenterCrop(128): это функция, которая обрезает центральную часть изображения до размера 128x128 пикселей.\n",
    "#           Все изображения должны иметь один и тот же размер перед подачей их на вход нейросети\n",
    "#    - transforms.ToTensor(): это преобразование изображений в формат torch.Tensor и нормализация значения пикселей к диапазону [0, 1]. (p / 255)\n",
    "#    - transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]): это преобразование нормализует каждый канал изображения (Red, Green, Blue),\n",
    "#           отняв среднее (0.5) и поделив на стандартное отклонение (0.5). Это делается для упрощения процесса обучения.\n",
    "\n",
    "train_dataset = datasets.CelebA(\n",
    "    root=\"/Users/martha-ezer/Desktop/dataset2\",\n",
    "    split=\"train\",\n",
    "    transform=transforms.Compose(\n",
    "        [\n",
    "            transforms.CenterCrop(128),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "        ]\n",
    "    ),\n",
    "    download=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Этот блок кода аналогичен прошлому, но в этот раз он загружает тестовый набор данных (вместо обучающего набора данных) из датасета CelebA.\n",
    "\n",
    "test_dataset = datasets.CelebA(\n",
    "    root=\"/Users/martha-ezer/Desktop/dataset2\",\n",
    "    split=\"test\",\n",
    "    transform=transforms.Compose(\n",
    "        [\n",
    "            transforms.CenterCrop(128),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "        ]\n",
    "    ),\n",
    "    download=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем два объекта DataLoader для обучающего и тестового набора данных соответственно,\n",
    "# используются в процессе обучения и тестирования модели для итерации по датасетам.\n",
    "\n",
    "# 1. dataset: это датасет, который будет использоваться. Здесь мы используем train_dataset для обучения и test_dataset для тестирования.\n",
    "# 2. shuffle: перемешивание данных в произвольном порядке при каждой новой эпохе обучения.\n",
    "#       Делается для обучающего набора данных для обеспечения лучшего обучения модели.\n",
    "#       Для тестового набора данных используется shuffle=False, потому что порядок данных не важен при тестировании.\n",
    "# 3. batch_size: это количество образцов, которые будут пропущены через модель за одну итерацию.\n",
    "# 4. num_workers: это количество подпроцессов, используемых для загрузки данных.\n",
    "\n",
    "import torch\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset, shuffle=True, batch_size=128, num_workers=4\n",
    ")\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=test_dataset, shuffle=False, batch_size=128, num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# slimnet 2017 модель\n",
    "# Далее определяется архитектура нейронной сети для задачи классификации.\n",
    "# Есть несколько определенных слоев:\n",
    "# 1. ConvBNReLU: Это простой слой, который объединяет\n",
    "#       свертку (Conv), нормализацию по пакету (BN - batch normalization) и функцию активации ReLU в один модуль.\n",
    "# 2. DWSeparableConv: Это слой глубокой свертки (depthwise separable convolution). Это более эффективный тип свертки.\n",
    "# 3. SSEBlock: Это слой, который использует предыдущие два для создания более сложной структуры.\n",
    "# 4. SlimModule: Это слой, который использует вышеописанные элементы для создания еще более сложного блока.\n",
    "# В SlimNet вся эта структура используется для создания конечной модели нейронной сети.\n",
    "# И создается экземпляр модели с 40 выходными классами и переносим ее на процессор (CPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Класс ConvBNReLU является подклассом nn.Sequential (объектом из пакета PyTorch, представляющим последовательность слоев).\n",
    "#   Он позволяет делать прямой проход через все слои последовательно.\n",
    "#   Этот класс представляет собой конвейер из трех слоев:\n",
    "#       сверточного слоя nn.Conv2d,\n",
    "#       слоя нормализации по батчу nn.BatchNorm2d и\n",
    "#       функции активации ReLU nn.ReLU.\n",
    "# ConvBNReLU - это обертка для группировки трех часто используемых вместе операций:\n",
    "#   свертки, батч-нормализации и функции активации ReLU. Это упрощает код и повышает его читаемость.\n",
    "class ConvBNReLU(nn.Sequential):\n",
    "    # in_planes - число входных каналов для сверточного слоя\n",
    "    # out_planes - число выходных каналов для сверточного слоя\n",
    "    # kernel_size - размер свертки - определяет размер \"окна\", которое движется по входному изображению.\n",
    "    #       Значение \"3\" означает, что используется окно размером 3x3 пиксела\n",
    "    #       Это наиболее распространенное значение для сверточных слоев, так как оно подходит для выявления большинства типичных паттернов\n",
    "    #       на изображениях, но не слишком велико, чтобы стать причиной переобучения\n",
    "    # stride - шаг свертки - определяет, во сколько пикселей смещается окно свертки после каждого вычисления.\n",
    "    #       Значение \"1\" означает, что окно смещается на один пиксель на каждом шаге\n",
    "    # groups - количество групп, на которые разделены входные данные\n",
    "    #       Значение \"1\" означает, что все входные каналы обрабатываются вместе, что является стандартной конфигурацией сверточного слоя.\n",
    "    #       Если бы groups было равно 2, входные каналы были бы разделены на две группы, и для каждой группы вычислялись бы свои свертки.\n",
    "    #       Это обычно используется для более сложных архитектур.\n",
    "    def __init__(self, in_planes, out_planes, kernel_size=3, stride=1, groups=1):\n",
    "        # padding = (kernel_size - 1) // 2 - Вычисление количества \"отступа\" (padding),\n",
    "        # которое должно быть добавлено к изображению при применении свертки так, чтобы размеры изображения оставались неизменными.\n",
    "        padding = (kernel_size - 1) // 2\n",
    "        # Аргументы передаваемые в super().init(...) - это слои, которые должны быть добавлены в наш Sequential объект\n",
    "        super(ConvBNReLU, self).__init__(\n",
    "            # Это сверточный слой. nn.Conv2d применяет двумерную свертку к входному изображению.\n",
    "            nn.Conv2d(\n",
    "                in_planes,\n",
    "                out_planes,\n",
    "                kernel_size,\n",
    "                stride,\n",
    "                padding,\n",
    "                groups=groups,\n",
    "                bias=False,\n",
    "            ),\n",
    "            # Это слой батч-нормализации. nn.BatchNorm2d нормализует выходное изображение слоя свёртки так,\n",
    "            # чтобы у каждого был нулевой средний и единичное стандартное отклонение.\n",
    "            # Используется для ускорения обучения и повышения его стабильности.\n",
    "            nn.BatchNorm2d(out_planes),\n",
    "            # Это функция активации ReLU. nn.ReLU() применяет функцию активации ReLU к нормализованному изображению.\n",
    "            nn.ReLU(inplace=True),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Класс DWSeparableConv - глубокий разделяемый сверточный слой (Depthwise Separable Convolutions).\n",
    "# Используется для упрощения и ускорения работы сети за счет разделения свертки на две части:\n",
    "#   глубинную свертку, обрабатывающей каждый канал отдельно (depthwise convolution) и\n",
    "#   свертку с равномерными весами, комбинирующей результаты в нужное количество каналов (pointwise convolution).\n",
    "# Наследуется от nn.Module - это основной класс в PyTorch для всех нейронных сетей и слоев.\n",
    "class DWSeparableConv(nn.Module):\n",
    "    # inp - число входных каналов\n",
    "    # out - число выходных каналов\n",
    "    def __init__(self, inp, out):\n",
    "        super().__init__()\n",
    "        # Создание глубинного сверточного слоя, где каждый входной канал обрабатывается отдельно.\n",
    "        # Другими словами, здесь inp групп сверток по одной на каждый входной канал. В результате размерность выхода остается той же, что и входа.\n",
    "        self.dwc = ConvBNReLU(inp, inp, kernel_size=3, groups=inp)\n",
    "        # Создание сверточного слоя с равномерными весами.\n",
    "        # Этот слой использует свертку 1x1, чтобы объединить выходы глубинного сверточного слоя в out каналов.\n",
    "        self.pwc = ConvBNReLU(inp, out, kernel_size=1)\n",
    "\n",
    "    # Функция, описывающая, как данные пропускаются через модель.\n",
    "    # В данном случае, она определяет, что вход x последовательно проходит через глубинную свертку и свертку с равномерными весами.\n",
    "    def forward(self, x):\n",
    "        # Применение глубинной свертки (dwc) и свертки с равномерными весами (pwc) к входным данным.\n",
    "        x = self.dwc(x)\n",
    "        x = self.pwc(x)\n",
    "        # Результат после прогона входных данных через оба слоя\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Этот код определяет класс SSEBlock, который является архитектурой блока нейронной сети, использующего\n",
    "#   свертки (Conv), батч-нормализацию (BN) и функции активации ReLU, а также глубинно разделяемые свертки (Depthwise Separable Convolution).\n",
    "# В контексте полной модели, SSEBlock представляет собой один из составных блоков этой модели,\n",
    "#   который преобразует входные данные и генерирует промежуточные выходные данные для следующих слоев или блоков модели.\n",
    "# Как правило, эти конструкции помогают упростить процесс обучения и упорядочить структуру модели.\n",
    "class SSEBlock(nn.Module):\n",
    "    # inp - число входных каналов\n",
    "    # out - число выходных каналов\n",
    "    def __init__(self, inp, out):\n",
    "        super().__init__()\n",
    "        # определяется переменная out_channel, назначенная числом, равным четырёмкратному количеству выходных каналов (oup).\n",
    "        # Такой подход применяется для обеспечения большего количества признаковых карт (feature maps) на выходе этих сверточных слоев.\n",
    "        out_channel = out * 4\n",
    "        # определение двух прямых сверточных слоев (pwc1 и pwc2) с батч-нормализацией и функцией активации ReLU.\n",
    "        self.pwc1 = ConvBNReLU(inp, out, kernel_size=1)\n",
    "        self.pwc2 = ConvBNReLU(out, out_channel, kernel_size=1)\n",
    "        # создание глубинного разделяемого сверточного слоя, который по сути является двумя сверточными слоями, работающими последовательно.\n",
    "        self.dwc = DWSeparableConv(out, out_channel)\n",
    "\n",
    "    # метод, определяющий прямой проход данных через нейросеть.\n",
    "    def forward(self, x):\n",
    "        # применение первого сверточного слоя pwc1 к x.\n",
    "        x = self.pwc1(x)\n",
    "        # применение второго сверточного слоя pwc2 и глубинно разделяемого сверточного слоя dwc к результату предыдущего слоя.\n",
    "        out1 = self.pwc2(x)\n",
    "        out2 = self.dwc(x)\n",
    "        # склеивание выходов двух последних слоев вдоль второй оси (axis=1) и возвращение результата в качестве выходных данных блока.\n",
    "        return torch.cat((out1, out2), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SlimModule, который является архитектурой блока нейронной сети, использующего\n",
    "#   SSEBlock, глубинно разделяемые свертки (DWSeparableConv), и свёртку с батч-нормализацией и функцией активации ReLU (ConvBNReLU).\n",
    "# SlimModule - это блок, составленный из других блоков и слоев для обучения сложных признаков из входных данных.\n",
    "# Сперва данные пропускаются через SSEBlock, затем складываются с выходом простого свёрточного слоя, снова пропускаются через SSEBlock и,\n",
    "#   в конце, через глубинно разделяемый сверточный слой.\n",
    "# Эта последовательность операций обеспечивает значительную способность обучения сложным признакам.\n",
    "# Данные складываются поэлементно: out += self.conv(x) - это пример операции \"остатка\" (residual operation),\n",
    "#   которая обычно помогает улучшить обратное распространение градиента и обеспечивает более стабильное обучение.\n",
    "class SlimModule(nn.Module):\n",
    "    # inp - число входных каналов\n",
    "    # oup - число выходных каналов\n",
    "    def __init__(self, inp, oup):\n",
    "        super().__init__()\n",
    "        # Это количества скрытых и выходных каналов, соответственно, для последующих слоев.\n",
    "        # Значения выбраны в четыре и три раза больше исходного количества каналов.\n",
    "        hidden_dim = oup * 4\n",
    "        out_channel = oup * 3\n",
    "        # определение двух блоков SSEBlock, которые представляют собой своего рода мини-нейронные сети, дополняющие этот модуль.\n",
    "        self.sse1 = SSEBlock(inp, oup)\n",
    "        self.sse2 = SSEBlock(hidden_dim * 2, oup)\n",
    "        self.dwc = DWSeparableConv(hidden_dim * 2, out_channel)\n",
    "        self.conv = ConvBNReLU(inp, hidden_dim * 2, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # пропускание входных данных через первый SSEBlock, слой ConvBNReLU, второй SSEBlock и DWSeparableConv соответственно.\n",
    "        out = self.sse1(x)\n",
    "        out += self.conv(x)\n",
    "        out = self.sse2(out)\n",
    "        out = self.dwc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Класс SlimNet определяет архитектуру сложной нейронной сети с использованием внутри составляющих компонентов.\n",
    "# Данная архитектура сети предназначена для работы со сложными изображениями,\n",
    "#   извлечения признаков с помощью SlimModule,\n",
    "#   сокращения размерности сокращение размерности данных с помощью макс-пулинга и\n",
    "#   глобального усреднения пулинга.\n",
    "# Это не только упрощает обработку данных, но и избавляет от избыточных пространственных информаций, сокращая риск переобучения.\n",
    "# После слоя глобального пулинга выходные данные становятся наиболее информативными изображениями в виде одномерных тензоров,\n",
    "#   содержащих существенную информацию о каждом изображении, который затем передается в полносвязный слой (self.fc).\n",
    "# Полносвязный слой действует как классификатор, который использует признаки, выделенные предыдущими слоями,\n",
    "#   для получения окончательных предсказаний изображений по классам.\n",
    "class SlimNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        # создание сверточного слоя с батчем-нормализацией и активацией ReLU.\n",
    "        # У данного слоя 3 входных канала (RGB), 96 выходных каналов, размер ядра 7x7 и шаг (stride) равный 2.\n",
    "        self.conv = ConvBNReLU(3, 96, kernel_size=7, stride=2)\n",
    "        # создание слоя пулинга (pooling), который применяется после сверточного слоя для уменьшения размерности данных.\n",
    "        # Это делает модель более устойчивой к изменениям входных данных. В данном случае используется макс-пулинг с размером ядра 3x3 и шагом 2.\n",
    "        self.max_pool0 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "\n",
    "        # Создание нескольких SlimModule и следующих за ними слоев макс-пулинга:\n",
    "        #   Эти модули являются кастомными блоками нейросети, определенные ранее.\n",
    "        #   Они обрабатывают входящие данные и увеличивают сложность модели.\n",
    "        #   Затем за каждым модулем следует слой макс-пулинга для уменьшения размерности.\n",
    "        self.module1 = SlimModule(96, 16)\n",
    "        self.module2 = SlimModule(48, 32)\n",
    "        self.module3 = SlimModule(96, 48)\n",
    "        self.module4 = SlimModule(144, 64)\n",
    "\n",
    "        self.max_pool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.max_pool2 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.max_pool3 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.max_pool4 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        # создание слоя глобального пулинга, который сжимает размерность (высоту и ширину) каждого входного канала до 1х1.\n",
    "        self.gap = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        # создание полносвязного слоя с 192 входами и выходами, кол-во которых равно числу классов.\n",
    "        self.fc = nn.Linear(192, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Затем входные данные отправляются в SlimModule. За каждым из них следует слой макс-пулинга.\n",
    "        # Это решение помогает уменьшить размерность признаковых карт и уменьшить переобучение.\n",
    "        x = self.max_pool0(self.conv(x))\n",
    "        x = self.max_pool1(self.module1(x))\n",
    "        x = self.max_pool2(self.module2(x))\n",
    "        x = self.max_pool3(self.module3(x))\n",
    "        x = self.max_pool4(self.module4(x))\n",
    "        # применяется глобальное усреднение пулинга\n",
    "        x = self.gap(x)\n",
    "        # входные данные \"расплющиваются\", то есть преобразуются в одномерный тензор.\n",
    "        x = torch.flatten(x, 1)\n",
    "        # применяется полносвязный слой\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализация объекта device, указывающего, на каком устройстве будет выполняться вычисления — на ЦПУ (CPU).\n",
    "device = torch.device(\"cpu\")\n",
    "# Создание экземпляра модели SlimNet для задачи классификации на 40 классов. Применение метода .to(device=device) гарантирует, что все вычисления данной модели будут осуществляться на устройстве, заданном в device, то есть на ЦПУ. Это означает, что все тензоры и параметры модели будут храниться и обрабатываться на ЦПУ.\n",
    "model = SlimNet(num_classes=40).to(device=device)\n",
    "# При выполнении этого кода, важно убедиться, что у вас достаточно мощный ЦПУ для осуществления всех вычислений, поскольку сложные нейронные сети могут потребовать большого количества вычислительных ресурсов. Если есть доступ к ГПУ, рекомендуется использовать их для значительного ускорения процесса обучения и выполнения. Вместо \"cpu\" в коде выше вы можете указать \"cuda\" для выполнения на ГПУ (при наличии)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch : 9 training loss: 0.1926736063518541\n",
      "train accruacy is 91.5143914726301%\n",
      "For epoch : 9 test loss: 0.21591500222027996\n",
      "test accruacy is 90.34715960324617%\n",
      "For epoch : 10 training loss: 0.1902523389572572\n",
      "train accruacy is 91.6247004976347%\n",
      "For epoch : 10 test loss: 0.21585689487813894\n",
      "test accruacy is 90.39938382927562%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     21\u001b[0m target \u001b[38;5;241m=\u001b[39m target\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mDoubleTensor)\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m---> 23\u001b[0m score \u001b[38;5;241m=\u001b[39m model(data)\n\u001b[1;32m     24\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_criterion(score, target)\n\u001b[1;32m     25\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[10], line 94\u001b[0m, in \u001b[0;36mSlimNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     92\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_pool0(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv(x))\n\u001b[1;32m     93\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_pool1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule1(x))\n\u001b[0;32m---> 94\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_pool2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule2(x))\n\u001b[1;32m     95\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_pool3(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule3(x))\n\u001b[1;32m     96\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_pool4(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule4(x))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[10], line 64\u001b[0m, in \u001b[0;36mSlimModule.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 64\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msse1(x)\n\u001b[1;32m     65\u001b[0m     out \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv(x)\n\u001b[1;32m     66\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msse2(out)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[10], line 47\u001b[0m, in \u001b[0;36mSSEBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     46\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpwc1(x)\n\u001b[0;32m---> 47\u001b[0m     out1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpwc2(x)\n\u001b[1;32m     48\u001b[0m     out2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdwc(x)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat((out1, out2), \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    457\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Здесь выполняется обучение модели для задачи классификации.\n",
    "\n",
    "\n",
    "# функция потерь. BCEWithLogitsLoss() соответствует Binary Cross Entropy Loss, который используется для бинарных задач классификации.\n",
    "loss_criterion = nn.BCEWithLogitsLoss()\n",
    "# оптимизатор. Он использует метод Adam для обновления параметров модели на основе вычисленных градиентов.\n",
    "#   lr это скорость обучения, которая контролирует размер шага в каждом обновлении.\n",
    "# В отличие от методов, таких как градиентный спуск или RMSprop, Adam более устойчив к шуму, пропускам и другим нерегулярностям в данных.\n",
    "#   Это помогает в обучении на данных лиц, которые могут быть неоднородными и содержать различного рода вариации.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "best_acc = 0.90325\n",
    "seed = 18203861252700\n",
    "# Обучение длится 50 \"эпох\". Эпоха это один проход через весь набор данных.\n",
    "for epoch in range(50):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    total_train = 0  # Общее количество обучающих примеров\n",
    "    correct_train = 0  # Число правильно классифицированных обучающих примеров\n",
    "    running_loss = 0  # Накопленная ошибка обучения\n",
    "    running_test_loss = 0  # Накопленная ошибка на тестовых данных\n",
    "    total_test = 0  # Oбщее количество тестовых примеров\n",
    "    correct_test = 0  # Число правильно классифицированных примеров на тестовом наборе\n",
    "    # устанавливаем модель в режим обучения\n",
    "    model.train()\n",
    "    # Проходим через обучающие данные в нашем DataLoader'е\n",
    "    for data, target in train_dataloader:\n",
    "        data = data.to(device=device)\n",
    "        target = target.type(torch.DoubleTensor).to(device=device)\n",
    "        # Пропускаем каждый пакет данных через модель\n",
    "        score = model(data)\n",
    "        # вычисляем потерю\n",
    "        loss = loss_criterion(score, target)\n",
    "        running_loss += loss.item()\n",
    "        # сбрасываем градиенты\n",
    "        optimizer.zero_grad()\n",
    "        # затем обратно распространяем ошибку\n",
    "        loss.backward()\n",
    "        # делаем шаг оптимизации\n",
    "        optimizer.step()\n",
    "        sigmoid_logits = torch.sigmoid(score)\n",
    "        predictions = sigmoid_logits > 0.5\n",
    "        total_train += target.size(0) * target.size(1)\n",
    "        correct_train += (target.type(predictions.type()) == predictions).sum().item()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # проходим через test_dataloader, делаем предсказания для каждого пакета и считаем потери и общую точность\n",
    "        for batch_idx, (images, labels) in enumerate(test_dataloader):\n",
    "            images, labels = images.to(device), labels.type(torch.DoubleTensor).to(\n",
    "                device\n",
    "            )\n",
    "            logits = model.forward(images)\n",
    "            test_loss = loss_criterion(logits, labels)\n",
    "            running_test_loss += test_loss.item()\n",
    "            sigmoid_logits = torch.sigmoid(logits)\n",
    "            # Вычисляем прогнозы для нашего входного батча (predictions = sigmoid_logits > 0.5) и\n",
    "            # подсчитываем количество верно классифицированных примеров.\n",
    "            predictions = sigmoid_logits > 0.5\n",
    "            total_test += labels.size(0) * labels.size(1)\n",
    "            correct_test += (labels.int() == predictions.int()).sum().item()\n",
    "    test_acc = correct_test / total_test\n",
    "    # Если точность тестирования этой эпохи лучше предыдущей лучшей, мы сохраняем модель\n",
    "    if test_acc > best_acc:\n",
    "        best_acc = test_acc\n",
    "        torch.save(model, f\"model_{test_acc*100}.pt\")\n",
    "        print(\n",
    "            f\"For epoch : { epoch} training loss: {running_loss/len(train_dataloader)}\"\n",
    "        )\n",
    "        print(f\"train accruacy is {correct_train*100/total_train}%\")\n",
    "        print(\n",
    "            f\"For epoch : {epoch} test loss: {running_test_loss/len(test_dataloader)}\"\n",
    "        )\n",
    "        print(f\"test accruacy is {test_acc*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-4.9256e+00, -5.7584e+00, -5.0295e+00, -3.4966e+00, -7.4958e+00,\n",
      "         -4.9172e+00, -2.6805e+00, -3.0474e+00, -4.1944e+00, -4.8382e+00,\n",
      "          1.1892e-01, -5.0804e+00, -7.2426e+00, -3.9917e+00, -5.5468e+00,\n",
      "         -4.0909e+00, -4.9752e+00, -3.5909e+00, -5.4830e+00, -4.3440e+00,\n",
      "          3.8656e-01, -4.9591e+00, -2.2133e+00, -1.4369e+00,  1.2706e-01,\n",
      "         -4.5493e+00, -2.8335e+00, -2.8260e+00, -3.2546e+00, -9.1848e+00,\n",
      "         -4.6841e+00, -5.3648e+00, -4.3216e+00, -3.1212e+00, -2.8049e+00,\n",
      "          9.0830e-03, -4.8716e+00, -3.2995e+00, -7.1756e+00, -1.4882e+00]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[-4.9255738e+00 -5.7584176e+00 -5.0294933e+00 -3.4966037e+00\n",
      "  -7.4957557e+00 -4.9172301e+00 -2.6804578e+00 -3.0473750e+00\n",
      "  -4.1944222e+00 -4.8382230e+00  1.1892551e-01 -5.0804191e+00\n",
      "  -7.2426486e+00 -3.9917459e+00 -5.5467815e+00 -4.0908670e+00\n",
      "  -4.9751635e+00 -3.5909424e+00 -5.4830050e+00 -4.3439617e+00\n",
      "   3.8655975e-01 -4.9591417e+00 -2.2133341e+00 -1.4368612e+00\n",
      "   1.2705985e-01 -4.5493464e+00 -2.8335381e+00 -2.8260260e+00\n",
      "  -3.2545526e+00 -9.1848106e+00 -4.6841149e+00 -5.3648167e+00\n",
      "  -4.3216519e+00 -3.1212153e+00 -2.8048992e+00  9.0840273e-03\n",
      "  -4.8716111e+00 -3.2995043e+00 -7.1756463e+00 -1.4881994e+00]]\n",
      "Exported model has been tested with ONNXRuntime, and the result looks good!\n"
     ]
    }
   ],
   "source": [
    "# Экспортируем модель PyTorch в формат ONNX (Open Neural Network Exchange) и затем проверяет экспортированную модель с использованием ONNX Runtime.\n",
    "# 1. torch.load: Загружает модель PyTorch с помощью функции torch.load. В этом примере модель подгружается из файла model_90.39938382927562.pt.\n",
    "# 2. x = torch.randn(1, 3, 128, 128, requires_grad=True): Создается случайный тензор для использования в тестировании модели.\n",
    "# 3. torch.onnx.export: Экспортирует модель PyTorch в формат ONNX. Параметры включают модель, входные данные, имя файла выходного файла и другие настройки, такие как версию ONNX и имена входных и выходных данных.\n",
    "# 4. onnxruntime.InferenceSession: Создает сессию для запуска обученной модели с использованием ONNX Runtime.\n",
    "# 5. to_numpy: Вспомогательная функция для преобразования тензора PyTorch в массив NumPy.\n",
    "# 6. ort_outs = ort_session.run(None, ort_inputs): Вызывает экспортированную модель с входными данными и заполняет выходные данные.\n",
    "# 7. np.testing.assert_allclose: Сравнивает выходные данные от экспортированной модели и оригинальной модели PyTorch, для проверки того что они близки по значениям (с небольшим зазором).\n",
    "# Перевод моделей из одного формата в другой дает возможность развертывать свои модели в различных средах и на разной аппаратуре.\n",
    "\n",
    "import onnxruntime\n",
    "import numpy as np\n",
    "\n",
    "torch_model = torch.load(\"model_90.39938382927562.pt\", map_location=\"cpu\")\n",
    "torch_model.eval()\n",
    "x = torch.randn(1, 3, 128, 128, requires_grad=True)\n",
    "torch_out = torch_model(x)\n",
    "print(torch_out)\n",
    "torch.onnx.export(\n",
    "    torch_model,\n",
    "    x,\n",
    "    \"cpu.onnx\",\n",
    "    export_params=True,\n",
    "    opset_version=10,\n",
    "    do_constant_folding=True,\n",
    "    input_names=[\"input\"],\n",
    "    output_names=[\"output\"],\n",
    ")\n",
    "ort_session = onnxruntime.InferenceSession(\n",
    "    \"cpu.onnx\", providers=[\"CPUExecutionProvider\"]\n",
    ")\n",
    "\n",
    "\n",
    "def to_numpy(tensor):\n",
    "    return (\n",
    "        tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "    )\n",
    "\n",
    "\n",
    "ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(x)}\n",
    "ort_outs = ort_session.run(None, ort_inputs)\n",
    "print(ort_outs[0])\n",
    "np.testing.assert_allclose(to_numpy(torch_out), ort_outs[0], rtol=1e-03, atol=1e-05)\n",
    "\n",
    "print(\"Exported model has been tested with ONNXRuntime, and the result looks good!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n"
     ]
    }
   ],
   "source": [
    "# АНАЛИЗ ВИДЕО\n",
    "\n",
    "# Этот код выполняет следующие действия:\n",
    "# 1. Использует библиотеку MediaPipe для обнаружения лиц на видео (\"test_face3.mp4\").\n",
    "# 2. Для каждого обнаруженного лица, он извлекает изображение лица и предобрабатывает его (с помощью функции cv2_preprocess),\n",
    "#       что включает в себя изменение размера изображения, конвертацию цвета, нормализацию и изменение формы массива.\n",
    "# 3. Затем он производит инференцию с помощью модели ONNX на предварительно обработанном лице.\n",
    "# 4. Результат инференции преобразуется в массив булевых значений и индексируется с помощью массива атрибутов (list_attr_en),\n",
    "#       чтобы получить список атрибутов, которые соответствуют лицу.\n",
    "# 5. Вернувшись к исходному изображению видео, на нем рисуются прямоугольники вокруг обнаруженных лиц\n",
    "#   и отображается список атрибутов, обнаруженных для каждого лица.\n",
    "# 6. Процесс повторяется для каждого кадра в видео, и каждый измененный кадр записывается в новое видеофайл - 'output3.avi'.\n",
    "\n",
    "import onnxruntime\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "cap = cv2.VideoCapture(\"test_face3.mp4\")\n",
    "ort_session = onnxruntime.InferenceSession(\"cpu.onnx\")\n",
    "list_attr_en = np.array(\n",
    "    [\n",
    "        \"5_o_Clock_Shadow\",\n",
    "        \"Arched_Eyebrows\",\n",
    "        \"Attractive\",\n",
    "        \"Bags_Under_Eyes\",\n",
    "        \"Bald\",\n",
    "        \"Bangs\",\n",
    "        \"Big_Lips\",\n",
    "        \"Big_Nose\",\n",
    "        \"Black_Hair\",\n",
    "        \"Blond_Hair\",\n",
    "        \"Blurry\",\n",
    "        \"Brown_Hair\",\n",
    "        \"Bushy_Eyebrows\",\n",
    "        \"Chubby\",\n",
    "        \"Double_Chin\",\n",
    "        \"Eyeglasses\",\n",
    "        \"Goatee\",\n",
    "        \"Gray_Hair\",\n",
    "        \"Heavy_Makeup\",\n",
    "        \"High_Cheekbones\",\n",
    "        \"Male\",\n",
    "        \"Mouth_Slightly_Open\",\n",
    "        \"Mustache\",\n",
    "        \"Narrow_Eyes\",\n",
    "        \"No_Beard\",\n",
    "        \"Oval_Face\",\n",
    "        \"Pale_Skin\",\n",
    "        \"Pointy_Nose\",\n",
    "        \"Receding_Hairline\",\n",
    "        \"Rosy_Cheeks\",\n",
    "        \"Sideburns\",\n",
    "        \"Smiling\",\n",
    "        \"Straight_Hair\",\n",
    "        \"Wavy_Hair\",\n",
    "        \"Wearing_Earrings\",\n",
    "        \"Wearing_Hat\",\n",
    "        \"Wearing_Lipstick\",\n",
    "        \"Wearing_Necklace\",\n",
    "        \"Wearing_Necktie\",\n",
    "        \"Young\",\n",
    "    ]\n",
    ")\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "out = cv2.VideoWriter(\n",
    "    \"output3.avi\", cv2.VideoWriter_fourcc(*\"MJPG\"), fps, (width, height)\n",
    ")\n",
    "\n",
    "\n",
    "# обработка изображения\n",
    "def cv2_preprocess(img):\n",
    "    img = cv2.resize(img, (128, 128), interpolation=cv2.INTER_NEAREST)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    mean = [0.5, 0.5, 0.5]\n",
    "    std = [0.5, 0.5, 0.5]\n",
    "    img = (img / 255.0 - mean) / std\n",
    "    img = img.transpose((2, 0, 1))\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = np.ascontiguousarray(img, dtype=np.float32)\n",
    "    return img\n",
    "\n",
    "\n",
    "# Сигмоидная функция преобразует каждое входное значение в число в диапазоне от 0 до 1\n",
    "def sigmoid_array(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "# список атрибутов\n",
    "def result_inference(input_array):\n",
    "    ort_inputs = {ort_session.get_inputs()[0].name: input_array}\n",
    "    ort_outs = ort_session.run(None, ort_inputs)\n",
    "    possibility = sigmoid_array(ort_outs[0]) > 0.5\n",
    "    result = list_attr_en[possibility[0]]\n",
    "    return result\n",
    "\n",
    "\n",
    "with mp_face_detection.FaceDetection(\n",
    "    model_selection=1, min_detection_confidence=0.5\n",
    ") as face_detection:\n",
    "    while cap.isOpened():\n",
    "        a1 = time.time()\n",
    "        success, image = cap.read()\n",
    "        if not success:\n",
    "            print(\"Ignoring empty camera frame.\")\n",
    "            break\n",
    "        image.flags.writeable = False  # предотвратить случайные изменения исходного изображения во время обработки\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        results = face_detection.process(image)\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        image2 = image.copy()\n",
    "        if results.detections:\n",
    "            for detection in results.detections:\n",
    "                mp_drawing.draw_detection(image, detection)\n",
    "                image_rows, image_cols, _ = image.shape\n",
    "                location = detection.location_data.relative_bounding_box\n",
    "                start_point = mp_drawing._normalized_to_pixel_coordinates(\n",
    "                    location.xmin, location.ymin, image_cols, image_rows\n",
    "                )\n",
    "                end_point = mp_drawing._normalized_to_pixel_coordinates(\n",
    "                    location.xmin + location.width,\n",
    "                    location.ymin + location.height,\n",
    "                    image_cols,\n",
    "                    image_rows,\n",
    "                )\n",
    "                x1, y1 = start_point\n",
    "                x2, y2 = end_point\n",
    "                img_infer = image2[\n",
    "                    y1 - 70 : y2, x1 - 50 : x2 + 50\n",
    "                ].copy()  # область интереса\n",
    "                img_infer = cv2_preprocess(img_infer)\n",
    "                result = result_inference(img_infer)\n",
    "                for i in range(0, len(result)):\n",
    "                    image = cv2.putText(\n",
    "                        image,\n",
    "                        result[i],\n",
    "                        (x1, y1 + i * 40),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                        1,\n",
    "                        (255, 255, 255),\n",
    "                        1,\n",
    "                        cv2.LINE_AA,\n",
    "                    )\n",
    "            a2 = time.time()\n",
    "        out.write(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# АНАЛИЗ ИЗОБРАЖЕНИЯ\n",
    "def process_image(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    with mp_face_detection.FaceDetection(\n",
    "        model_selection=1, min_detection_confidence=0.5\n",
    "    ) as face_detection:\n",
    "        image.flags.writeable = False\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        results = face_detection.process(image)\n",
    "        if results is not None:\n",
    "            image.flags.writeable = True\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "            image2 = image.copy()\n",
    "            if results.detections:\n",
    "                for detection in results.detections:\n",
    "                    mp_drawing.draw_detection(image, detection)\n",
    "                    image_rows, image_cols, _ = image.shape\n",
    "                    location = detection.location_data.relative_bounding_box\n",
    "                    start_point = mp_drawing._normalized_to_pixel_coordinates(\n",
    "                        location.xmin, location.ymin, image_cols, image_rows\n",
    "                    )\n",
    "                    end_point = mp_drawing._normalized_to_pixel_coordinates(\n",
    "                        location.xmin + location.width,\n",
    "                        location.ymin + location.height,\n",
    "                        image_cols,\n",
    "                        image_rows,\n",
    "                    )\n",
    "                    x1, y1 = start_point\n",
    "                    x2, y2 = end_point\n",
    "                    img_infer = image2[y1 - 70 : y2, x1 - 50 : x2 + 50].copy()\n",
    "                    img_infer = cv2_preprocess(img_infer)\n",
    "                    result = result_inference(img_infer)\n",
    "                    for i in range(0, len(result)):\n",
    "                        image = cv2.putText(\n",
    "                            image,\n",
    "                            result[i],\n",
    "                            (x1, y1 + i * 40),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                            1,\n",
    "                            (255, 255, 255),\n",
    "                            1,\n",
    "                            cv2.LINE_AA,\n",
    "                        )\n",
    "                cv2.imwrite(\"output.jpg\", image)\n",
    "\n",
    "\n",
    "process_image(\"test_img.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# АНАЛИЗ С ВЕБКАМЕРЫ В РЕЖИМЕ РЕАЛЬНОГО ВРЕМЕНИ\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "with mp_face_detection.FaceDetection(\n",
    "    model_selection=1, min_detection_confidence=0.5\n",
    ") as face_detection:\n",
    "    while cap.isOpened():\n",
    "        a1 = time.time()\n",
    "        success, image = cap.read()\n",
    "        if not success:\n",
    "            print(\"Ignoring empty camera frame.\")\n",
    "            break\n",
    "        image.flags.writeable = False\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        results = face_detection.process(image)\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        image2 = image.copy()\n",
    "        if results.detections:\n",
    "            for detection in results.detections:\n",
    "                mp_drawing.draw_detection(image, detection)\n",
    "                image_rows, image_cols, _ = image.shape\n",
    "                location = detection.location_data.relative_bounding_box\n",
    "                start_point = mp_drawing._normalized_to_pixel_coordinates(\n",
    "                    location.xmin, location.ymin, image_cols, image_rows\n",
    "                )\n",
    "                end_point = mp_drawing._normalized_to_pixel_coordinates(\n",
    "                    location.xmin + location.width,\n",
    "                    location.ymin + location.height,\n",
    "                    image_cols,\n",
    "                    image_rows,\n",
    "                )\n",
    "\n",
    "                if end_point is not None:\n",
    "                    x2, y2 = end_point\n",
    "                    x1, y1 = start_point[0], start_point[1]\n",
    "\n",
    "                    # Добавляем проверку на размер изображения\n",
    "                    y1_new = max(0, y1 - 70)\n",
    "                    y2_new = min(image.shape[0], y2)\n",
    "                    x1_new = max(0, x1 - 50)\n",
    "                    x2_new = min(image.shape[1], x2 + 50)\n",
    "\n",
    "                    img_infer = image2[y1_new:y2_new, x1_new:x2_new].copy()\n",
    "\n",
    "                    # Проверяем, что изображение не пустое\n",
    "                    if img_infer.size != 0:\n",
    "                        img_infer = cv2_preprocess(img_infer)\n",
    "                        result = result_inference(img_infer)\n",
    "                        for i in range(0, len(result)):\n",
    "                            image = cv2.putText(\n",
    "                                image,\n",
    "                                result[i],\n",
    "                                (x1, y1 + i * 40),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                                1,\n",
    "                                (255, 255, 255),\n",
    "                                1,\n",
    "                                cv2.LINE_AA,\n",
    "                            )\n",
    "\n",
    "        cv2.imshow(\"Frame\", image)\n",
    "        a2 = time.time()\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "cap.release()\n",
    "cv2.imshow(\"Frame\", image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
